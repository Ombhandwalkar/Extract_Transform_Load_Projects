# ğŸ› ï¸ Data Engineering Workflow â€“ Customer Transactions Pipeline

## ğŸ“Œ Overview
This project demonstrates an **end-to-end data engineering workflow** for processing **customer transactions and labeled data**.  
It showcases how to build a modern data pipeline with **Apache Airflow**, **Python**, **PostgreSQL**, **MySQL**, **Amazon S3**, **Snowflake**, and **SODA** for data quality checks.

The pipeline covers:
1. **Triggers** â€“ Generate transaction & detection data using Python.  
2. **Write** â€“ Store raw and labeled data into S3 and load them into the Data Warehouse (Snowflake).  
3. **Audit** â€“ Run **SODA** quality checks to ensure clean and reliable data.  
4. **Publish** â€“ Create customer metrics tables for dashboards and reporting.  

---

## ğŸ¥ Demo Video

[![Demo Video](https://img.youtube.com/vi/jDvIzTbG5zk/0.jpg)](https://youtu.be/jDvIzTbG5zk)


## âš™ï¸ Workflow Architecture

![alt text](<workflow.png>)


## ğŸš€ Components Used
### 1.  **Data Generation (Triggers)**

* transaction_generator.py â†’ produces synthetic customer transactions.

* detection_generator.py â†’ produces labeled transactions.

* Data is stored in PostgreSQL and MySQL.

### 2. **Data Storage (Write)**

* Data from source databases is ingested into Amazon S3 (raw zone).

* From S3, data is loaded into Snowflake for analytics.

### 3. **Data Quality (Audit)**

* SODA runs data quality checks on Snowflake tables.

* Ensures schema validation, null checks, and data consistency.

### 4. **Data Transformation**

* Transformations create customer metrics tables in Snowflake by using DBT.



## âš™ï¸ Automated DAG(Directed Acyclic Graph) Architecture
![alt text](<dag.png>)
